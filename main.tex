\documentclass[10pt, a4paper]{article}
\usepackage{lrec}
%\usepackage{multibib}
%\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{todonotes}
% for eps graphics

\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{xstring}

\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}

% google doc: https://docs.google.com/document/d/1czT8UndVGbKUfSRZtaHVyWbpFcGSV33USwytefEv4jM/edit?usp=sharing


\title{Build-your-own is better than public text representation models: evidence for the Basque Language}

\name{Author1, Author2, Author3}

\address{Affiliation1, Affiliation2, Affiliation3 \\
         Address1, Address2, Address3 \\
         author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\
         \{author1, author5, author9\}@abc.org\\}


\abstract{
% Ondorio nagusiak:
% Euskararako bert propioa > mbert publikoa eta euskararako fastext propioa > fastext publikoa eta flair propioa > flair publikoa
% Atazak: topic class / polarity / ner / pos
% Artearen egoera ataza guztietan (konprobatu bakoitzak berea)
% Euskaraz: bert > flair > fastext (単abardurekin idatzi)
% Embedding-ak eta ereduak banatuko ditugu
% Honek 単abardura gehitzen dio alemanari (japonesa?) buruzko lanari (jon ander) \\ \newline \Keywords{keyword1, keyword2, keyword3}
}

\begin{document}

\maketitleabstract

\section{Introduction}\label{sec:introduction}

\todo[inline]{abstract/intro enekok}


\section{Related work}\label{sec:related-work}

% embedding / bert / mbert
\todo[inline]{related work aitor hasi. camembert aipatu?}


\section{Building Basque models}\label{sec:build-basq-models}

% - Collecting corpora: i単aki
\subsection{Collecting Corpora}\label{sec:build-basq-models:corpora}
 
 We collected a corpus comprising the Basque Wikipedia\footnote{The dump from 2019/10/01 was used.}, and news articles crawled from Basque media. In total it contains 226 million tokens, with 35 millions coming from Wikipedia. We will refer to this corpus as Basque Media Corpus (BMC).
 
 \todo[inline]{Justificatu zergatik ez common crawl. Aipatu iturriak. justifikatu zergatik ezin dugun publiko jarri}


 
% - fastext i単aki/xabier
\subsection{Static Embeddings: FastText}\label{sec:build-basq-models:static}

To the best of our knowledge, the only publicly available static word embeddings for basque are those distributed by Facebook computed from two sources by using The FastText algorithm \cite{}:
\begin{itemize}
    \item Wikipedia (wiki-ft) based embeddings \cite{fasttext1_bojanowski2017enriching} (skipgram,d300,mf5,char3\_6,w5,e5,negs5)
    \item Common Crawl and Wikipedia based embeddings (cc-ft) \cite{fasttext2_grave2018learning}
\end{itemize} and cc-wiki (cbow,char\_5-5,w5,e5,negs10)
Ours: default original paper (ref. needed) -  (cbow,d300,mf5,char3\_6,w5,e5,negs5)
Ablation tests?

Mention to word2vec?


% - Flair rodri
\subsection{Contextual String Embeddings: Flair}\label{sec:build-basq-models:flair}

Static word embeddings such as FastText \cite{fasttext1_bojanowski2017enriching} provide a unique vector-based representation for a given word independently of the context in which the word occurs. Thus, if we consider the Basque word \emph{banku}\footnote{In English: bank.}, static word embedding approaches will calculate one vector irrespective of the fact that the same word \emph{banku} may convey in some contexts different senses, namely, ``financial institution'',``bench'', ``supply or stock'', among others. This problem has been recently addressed by the so-called contextual word embeddings with the objective of being able to generate different word representations according to the context in which the word appears. Example of such contextual representations are ELMO \cite{Peters:2018} and Flair \cite{akbik2018coling}.

Flair refers to both a deep learning system and to a specific type of character-based contextual word embeddings. Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks \cite{akbik2018coling}, outperforming other well-known approaches such as BERT and ELMO \cite{devlin2019bert,Peters:2018}. In any case, Flair is of interest to us because they distribute their own Basque pre-trained embedding models obtained from a corpus of 36M tokens (combining OPUS and Wikipedia).

\paragraph{Flair-BMC models:} We train our own Flair embeddings using the BMC corpus with the following parameters: Hidden size 2048, sequence length of 250, and a mini-batch size of 100. The rest of the parameters are left in their default setting. Training was done for 5 epochs over the full training corpus. The training of each model took 48h on a Nvidia Titan V GPU.

Flair's embeddings model words as sequences of characters. Moreover, the vector-based representation of a word will depend on its surrounding context. More specifically, to generate word embeddings they feed sentences as sequences of characters into a character-level Long short-term memory (LSTM) model which at each point in the sequence is trained to predict the next character. Given a sentence, a forward LSTM language model processes the sequence from the beginning of the sentence to the last character of the word we are modeling extracting its output hidden state. A backward LSTM performs the same operation going from the end of the sentence up to the first character of the word. In this case, the extracted hidden state contains information propagated from the end of the sentence to this point. Both hidden states are concatenated to generate the final embedding.

The final word embeddings are passed into the BiLSTM-CRF sequence labeling architecture (Flair system) proposed by \cite{huang2015bidirectional}. Although for best results they recommend to stack their own Flair embeddings with additional static embeddings such as FastText, we will perform our evaluation in order to compare the official Flair embeddings for Basque with our own Flair-BMC embeddings.

% - Bert ander/jon ander
\subsection{BERT language models}\label{sec:build-basq-models:bert}

We have trained a BERT \cite{devlin2019bert} model for Basque Language using the BMC corpus motivated by the low representation this language has in the original multilingual BERT model. In this section we describe the methods used for creating the vocabulary, the model architecture, the pre-training objective and procedure.

The main differences between our model and the original implementation are the corpus used for the pre-training, the algorithm for sub-word vocabulary creation and the usage of a different masking strategy that is not available for the BERT\textsubscript{BASE} model yet. 

\paragraph{Sub-word vocabulary}

We create a cased sub-word vocabulary containing 50,000 tokens using the unigram language model based subword segmentation algorithm proposed by \newcite{kudo2018subword}. We do not use the same algorithm as BERT because the WordPiece \cite{wu2016google} implementation they originally used is not publicly available. We have increased the vocabulary size from 30,000 sub-word units up to 50,000 expecting to be beneficial for the Basque language due to its agglutinative nature. Our vocabulary is learned from the whole training corpus but we do not cover all the characters in order to avoid very rare ones. We set the coverage percentage to $99.95$. 

\paragraph{Model Architecture}

In the same way as the original BERT architecture proposed by \newcite{devlin2019bert} our model is composed by stacked layers of Transformer encoders \cite{vaswani2017attention}. Our approach follows the BERT\textsubscript{BASE} configuration containing 12 Transformer encoder layers, a hidden size of 768 and 12 self-attention heads for a total of 110M parameters. 

\paragraph{Pre-training objective}

Following BERT original implementation, we train our model on the Masked Language Model (MLM) and Next Sentence Prediction (NSP) tasks. Even if the necessity of the NSP task has been questioned by some recent works \cite{yang2019xlnet,liu2019roberta,lample2019cross} we have decided to keep it as in the original paper. For the MLM, given an input sequence composed of N tokens $x_1, x_2, ..., x_n$ we select a $15\%$ of them as masking candidates. Then, from this selected tokens a $80\%$ are masked by replacing them with the [MASK] token, a $10\%$ are replaced with a random word of the vocabulary and the remaining $10\%$ are left unchanged. In order to create input examples for the NSP task, we take two segments $A$ and $B$ from the training corpus, where $B$ is the true next segment for $A$ only in a $50\%$ of the cases, in the rest of the cases, $B$ is just a random segment from the corpus. At the end, the model is trained to optimize the sum of the means of the MLM and NSP likelihoods.

As our vocabulary consists of sub-word units, we use whole-word masking (WWM), that applies the masking before the sub-word tokenization. This new masking strategy makes the MLM task more difficult for the system as it has to predict the whole word instead of predicting just part of it. An upgraded version of BERT\textsubscript{LARGE}\footnote{\url{https://github.com/google-research/bert}} has proven that WWM has substantial benefits in comparison with previous masking that was done after the sub-word tokenization. 

\paragraph{Pre-training procedure}

Similar to \cite{devlin2019bert} we use Adam with learning rate of $1e-4$, $\beta_1=0.9$, $\beta_2=0.999$, L2 weight decay of $0.01$, learning rate warmup over the first $10,0000$ steps, and linear decay of the learning rate. The dropout probability is fixed to $0.1$ on all the layers. 

As the attentions is quadratic to the sequence length, making longer sequences much more expensive, we pre-train the model with sequence length of $128$ for $90\%$ of the steps and sequence length of $512$ for $10\%$ of the steps. In total we train for $1,000,000$ steps and a batch size of $256$. The first $90\%$ steps are trained using Cloud v2 TPUs and for the rest of the steps we use Cloud v3 TPUs \footnote{\url{https://cloud.google.com/tpu/pricing}}. 

\section{Experimental settings}\label{sec:exper-sett}

% - Datasets i単aki/xabier/rodri

\todo[inline]{Rodri: Nik atal hau kenduko nuke eta jarriko nituzke hemengo edukiak dagokien azpi-ataletan esperimentu/emaitzekin}

We conduct an extensive evaluation on four well known NLP tasks: Topic Classification, Sentiment Classification, Part-of-Speech (POS) tagging and Named Entity Recognition (NER). POS tagging and NER are evaluated on two establish benchmarks, i.e., Universal Dependencies 1.2 and the EIEC corpus.

For polarity and topic classification we introduce two new datasets for Basque. For sentiment classification a corpus of tweets containing messages related to the cultural domain was used \cite{san2019multilingual}\footnote{The dataset is publicly available at https://hizkuntzateknologiak.elhuyar.eus/assets/files/behaguneadss2016-dataset.tgz}. The corpus contains three class annotations (positive, negative and neutral). For the task of topic classification a dataset containing 12k news short texts was compiled. Each text is tagged as belonging to a single topic in a 12 class scheme


% - Text classification models i単aki/xabier
% - Sequence labeling models i単aki/rodri/jon ander

\section{Results and discussion}\label{sec:results-discussion}

nomenclatura: \begin{itemize}
    \item fastext-official-wikipedia
    \item fastext-official-common-crawl
    \item fastext-BMC
    \item flair-official
    \item flair-BMC
    \item mBERT-official
    \item BERT-BMC
\end{itemize}

\todo[inline]{ataza bakoitzeko taula bat eta atal bat, emaitza horiek komentatzen, artearen egoera barne.}


% - Topic classification xabier
\subsection{Topic classification}\label{sec:topic}

\begin{table}[!t]\scriptsize
\centering
\begin{tabular}{@{\hspace{0.3cm}}lcc} \hline
%\textbf{} & \multicolumn{4}{c}{\textbf{Task}} \\ %\hline
 {\textbf{}} & {\textbf{micro F1}} &  {\textbf{Macro F1}} \\ \hline
\multicolumn{3}{@{}l}{\textit{\scriptsize{Static Embeddings}}} \\
fastext-wikipedia &  &	 \\
fastext-common-crawl &  &   \\
fastext-BMC  &  &	 \\
\hline%\hline
\multicolumn{3}{@{}l}{\scriptsize{\textit{Contextual Embeddings}}}\\
Flair-official &  &	 \\
Flair-BMC  & 	&  \\
%\hline
%\multicolumn{5}{@{}l}{\scriptsize{\textit{BERT Embeddings}}}\\
mBERT-official  & 68.42 & 48.38  \\
BERT-BMC  & \textbf{76.77}	& \textbf{63.46}  \\
\hline 
%\multicolumn{3}{@{}l}{\scriptsize{\textit{Previous state-of-the-art}}} \\
%\hline
%- & - & -\\
\end{tabular}
\caption{Results for the topic classification task}\label{tab:topic}
\end{table}

% - Polarity i単aki
\subsection{Sentiment classification}\label{sec:polarity}

\begin{table}[!t]\scriptsize
\centering
\begin{tabular}{@{\hspace{0.3cm}}lcc} \hline
%\textbf{} & \multicolumn{4}{c}{\textbf{Task}} \\ %\hline 
 {\textbf{}} & {\textbf{micro F1}} &  {\textbf{Macro F1}} \\ \hline
\multicolumn{3}{@{}l}{\textit{\scriptsize{Static Embeddings}}} \\
fastext-wikipedia & 71.10 &	66.72 \\
fastext-common-crawl & 66.16 & 58.89  \\
fastext-BMC  & 72.19 &	68.14 \\
\hline%\hline
\multicolumn{3}{@{}l}{\scriptsize{\textit{Contextual Embeddings}}}\\
Flair-official & 69.93 & 64.06 \\
Flair-BMC  & 71.71	& 68.51 \\
%\hline
%\multicolumn{5}{@{}l}{\scriptsize{\textit{BERT Embeddings}}}\\
mBERT-official  & 71.02 & 66.02 \\
BERT-BMC  & \textbf{78.10}	& \textbf{76.14}  \\
\hline 
\multicolumn{3}{@{}l}{\scriptsize{\textit{Previous state-of-the-art}}} \\
\hline
SVM \cite{san2019multilingual} & 74.02 & 69.87\\
\end{tabular}
\caption{Results for the sentiment classification task}\label{tab:sentiment}
\end{table}
\todo[inline]{Behin behinekoa da taula, zalantza dugu xehetasun gehiago jarri (klase bakoitzeko f1 scoreak adib). Komeni da adostea denok? Edo taula hauetan bakoitzak bere egitura jarraitzea hobe da ataza bakoitzean nabarmentzeko gauzak ezberdinak izan daitezkeelako?}



% - PoS tagging rodri/i単aki
% - NERC rodri

\subsection{POS Tagging}\label{sec:pos-tagging}

In order to facilitate comparison with previous state-of-the-art methods, we experiment with the Universal Dependencies 1.2 data, which provides train, development and test partitions. The Basque UD treebank \cite{aranzabe2015automatic} is based on a conversion from part of the Basque Dependency Treebank (BDT) \cite{aduriz2003construction}. The treebank consists of 5274 sentences (60563 tokens) and covers mainly literary and journalistic texts.

\begin{table}[!t]\footnotesize
\centering
\begin{tabular}{@{\hspace{0.3cm}}lccc} \hline
 \textbf{} & \textbf{Word Accuracy} \\ \hline
\textbf{Static Embeddings} & \\
FastText-Wikipedia &  \\
FastText-Common-Crawl & \\
FastText-BMC  & \\ \hline
\textbf{FlairEmbeddings}\\
Flair-official &  \\
Flair-BMC  &  \\ \hline
\textbf{BERT Language Models}\\
mBERT-official  &  \\
BERT-BMC  & \\ \hline
\textbf{Baseline} \\
\cite{plank-etal-2016-multilingual} & 95.51 \\ \hline
\end{tabular}
\caption{Basque POS tagging results on UD 1.2.}\label{tab:pos}
\end{table}

We train Flair using the parameters specified in \cite{akbik2019naacl}, tuning the system on the development data and using the test only for the final evaluation.
Table shows first the results obtained by training the system with the Flair character-based contextual embeddings only. After that we combine the Flair embeddings with FastTex embeddings (both official and BMC).


Furthermore, our best results establish new state-of-the-art performance for POS tagging using UD 1.2 for Basque.


\subsection{Named Entity Recognition}\label{sec:named-entity-recogn}

The only gold standard corpus for NER in Basque is EIEC\footnote{\url{http://ixa2.si.ehu.eus/eiec/eiec_v1.0.tgz}} \cite{alegria2006lessons}. The corpus contains 44K tokens for training (3817 unique entities) and 15K for testing (931 entities). Although EIEC is annotated with four entity types (Location, Person, Organization and Miscellaneous), the \emph{Miscellaneous} class is extremely sparse, occurring only in a proportion of 1 to 10 with respect to the other three classes. Thus, in the training data there are 156 entities annotated as \emph{Miscellaneous} whereas each of the other three classes contain around 1200 entities.

\begin{table}[!t]\footnotesize
\centering
\begin{tabular}{@{\hspace{0.3cm}}lccc} \hline
 \textbf{} &\textbf{Precision} & \textbf{Recall} & \textbf{F1} \\ \hline
\textbf{Static Embeddings} & & &  \\
FastText-Wikipedia & & & \\
FastText-Common-Crawl &  & &  \\
FastText-BMC  &  & &  \\
\hline%\hline
\textbf{Flair embeddings}\\
Flair-official &  &  &   \\
Flair-BMC & & &  \\ \hline
\textbf{BERT Language Models} \\
mBERT-official  &  & &   \\
BERT-BMC  &  & &   \\ \hline
\textbf{Baseline} \\
\cite{agerri2016robust} & 80.66 & 73.14 & 76.72 \\ \hline
\end{tabular}
\caption{Basque NER results on EIEC corpus.}\label{tab:ner}
\end{table}


\section{Discussion}\label{sec:discussion}

\begin{table*}[!t]\scriptsize
\centering
\begin{tabular}{@{\hspace{0.3cm}}lcccc} \hline
\textbf{} & \multicolumn{4}{c}{\textbf{Task}} \\ %\hline
 & {\textbf{Topic Classification}} & {\textbf{Polarity}} &  {\textbf{POS}} & {\textbf{NER}}\\ \hline
\multicolumn{5}{@{}l}{\textit{\scriptsize{Static Embeddings}}} \\
fastext-wikipedia & & & & \\
fastext-common-crawl & & & &  \\
fastext-BMC  & & & &  \\
\hline%\hline
\multicolumn{5}{@{}l}{\scriptsize{\textit{Contextual Embeddings}}}\\
Flair-official & & & &  \\
Flair-BMC  & & & &  \\
%\hline
%\multicolumn{5}{@{}l}{\scriptsize{\textit{BERT Embeddings}}}\\
mBERT-official  & & & &  \\
BERT-BMC  & & & &  \\ \hline
\multicolumn{5}{@{}l}{\scriptsize{\textit{Previous state-of-the-art}}} \\
\hline
\end{tabular}
\caption{Summary table across all tasks. Micro F1 scores are reported}\label{sec:results-discussion:table}
\end{table*}

\todo[inline]{discussion over summary table}
% - discussion



\section{Conclusions and future work}\label{sec:concl-future-work}

\section*{Acknowledgments}

This work has been funded by the~Spanish Ministry of Science, Innovation and Universities under the project DeepReading (RTI2018-096846-B-C21) (MCIU/AEI/FEDER, UE) and by the BBVA Big Data 2018 ``BigKnowledge for Text Mining (BigKnowledge)'' project.
%The XX author is funded by the Ramon y Cajal Fellowship RYC-2017-23647. We also acknowledge the~support of the NVIDIA Corporation with the~donation of a Titan V GPU used for this research.

\section{Bibliographical References}

\bibliographystyle{lrec}
\bibliography{main}

\end{document}
