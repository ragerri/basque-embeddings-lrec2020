\documentclass[10pt, a4paper]{article}
\usepackage{lrec}
%\usepackage{multibib}
%\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}
\usepackage{todonotes}
% for eps graphics

\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{xstring}

\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}

% google doc: https://docs.google.com/document/d/1czT8UndVGbKUfSRZtaHVyWbpFcGSV33USwytefEv4jM/edit?usp=sharing


\title{Build-your-own is better than public text representation models: evidence for Basque Language}

\name{Author1, Author2, Author3}

\address{Affiliation1, Affiliation2, Affiliation3 \\
         Address1, Address2, Address3 \\
         author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\
         \{author1, author5, author9\}@abc.org\\}


\abstract{
% Ondorio nagusiak: 
% Euskararako bert propioa > mbert publikoa eta euskararako fastext propioa > fastext publikoa eta flair propioa > flair publikoa
% Atazak: topic class / polarity / ner / pos
% Artearen egoera ataza guztietan (konprobatu bakoitzak berea)
% Euskaraz: bert > flair > fastext (ñabardurekin idatzi)
% Embedding-ak eta ereduak banatuko ditugu
% Honek ñabardura gehitzen dio alemanari (japonesa?) buruzko lanari (jon ander)
\\ \newline \Keywords{keyword1, keyword2,
keyword3} }

\begin{document}

\maketitleabstract

\section{Introduction}
\label{sec:introduction}

\todo[inline]{abstract/intro enekok}


\section{Related work}
\label{sec:related-work}

% embedding / bert / mbert
\todo[inline]{related work aitor hasi. camembert aipatu?}


\section{Building Basque models}
\label{sec:build-basq-models}

% - Collecting corpora: iñaki
\subsection{Collecting corpora}
\label{sec:build-basq-models:corpora}
 
 We collected a corpus comprising the Basque Wikipedia\footnote{The dump from 2019/10/01 was used.}, and news articles crawled from Basque media. In total it contains 226 million tokens, with 35 millions coming from Wikipedia. We will refer to this corpus as Basque Media Corpus (BMC).
 
 \todo[inline]{Justificatu zergatik ez common crawl. Aipatu iturriak. justifikatu zergatik ezin dugun publiko jarri}


 
% - fastext iñaki/xabier
\subsection{Static embeddings: FastText}
\label{sec:build-basq-models:static}

To the best of our knowledge, the only publicly available static word embeddings for basque are those distributed by Facebook computed from two sources by using The FastText algorithm \cite{}: 
\begin{itemize}
    \item Wikipedia (wiki-ft) based embeddings \cite{fasttext1_bojanowski2017enriching} (skipgram,d300,mf5,char3\_6,w5,e5,negs5)
    \item Common Crawl and Wikipedia based embeddings (cc-ft) \cite{fasttext2_grave2018learning}
\end{itemize} and cc-wiki (cbow,char\_5-5,w5,e5,negs10)
Ours: default original paper (ref. needed) -  (cbow,d300,mf5,char3\_6,w5,e5,negs5)
Ablation tests?

Mention to word2vec?


% - Flair rodri
\subsection{Contextual embeddings: Flair}
\label{sec:build-basq-models:flair}

\cite{akbik2018coling}


% - Bert ander/jon ander
\subsection{BERT language models}
\label{sec:build-basq-models:bert}



Sarreratxoa ataleko kontu nagusiak azalduz. 

\paragraph{Model Architecture}

In the same way as the original BERT architecture proposed by \newcite{devlin2018bert} our model is composed by stacked layers of Transformer encoders \cite{vaswani2017attention}. Our approach follows the BERT\textsubscript{BASE} configuration containing 12 Transformer encoder layers, a hidden size of 768 and 12 self-attention heads for a total of 110M parameters. 

Gainetik - BERT base erabiltzen dugula esan - transformer encoderra ezabili.

\paragraph{Pretraining objective}

MaskedLM and NSP azaldu//Whole work masking - Arazoa NSP ez da erabiltzen azken modeloetan RoBERTA, XLM...


\paragraph{Vocabulary}

SentencePiece azaldu,  gure vocab size 50K

\paragraph{Pretraining procedure}

Optimization aipatu, Cloud v2 TPUs for 128 length, v3 for 512 length - 90pstep 128length 10pstep 512 length - total 100000 step 3 egun


\section{Experimental settings}
\label{sec:exper-sett}

% - Datasets iñaki/xabier/rodri
We conduct an extensive evaluation on four well known NLP tasks: Topic Classification, Sentiment Classification, POS tagging, and Named Entity Recognition (NER). POS tagging and NER are evaluated on two establish benchmarks, i.e., Universal Dependencies and EIEC corpus. 

For polarity and topic classification we introduce two new datasets for Basque. For sentiment classification a corpus of tweets containing messages related to the cultural domain was used \cite{san2019multilingual}\footnote{The dataset is publicly available at https://hizkuntzateknologiak.elhuyar.eus/assets/files/behaguneadss2016-dataset.tgz}. The corpus contains three class annotations (positive, negative and neutral). For the task of topic classification a dataset containing 12k news short texts was compiled. Each text is tagged as belonging to a single topic in a 12 class scheme


% - Text classification models iñaki/xabier
% - Sequence labeling models iñaki/rodri/jon ander

\section{Results and discussion}
\label{sec:results-discussion}

nomenclatura: \begin{itemize}
    \item fastext-official-wikipedia 
    \item fastext-official-common-crawl 
    \item fastext-BMC
    \item flair-official
    \item flair-BMC
    \item mBERT-official
    \item BERT-BMC
\end{itemize}

\todo[inline]{ataza bakoitzeko taula bat eta atal bat, emaitza horiek komentatzen, artearen egoera barne.}


% - Topic classification xabier
% - Polarity iñaki
% - PoS tagging rodri/iñaki
% - NERC rodri

\begin{table*}[!t]\scriptsize
\centering
\begin{tabular}{@{\hspace{0.3cm}}lcccc} \hline
\textbf{} & \multicolumn{4}{c}{\textbf{Task}} \\ %\hline 
 & {\textbf{Topic Classification}} & {\textbf{Polarity}} &  {\textbf{POS}} & {\textbf{NER}}\\ \hline
\multicolumn{5}{@{}l}{\textit{\scriptsize{Static Embeddings}}} \\
fastext-wikipedia & & & & \\
fastext-common-crawl & & & &  \\
fastext-BMC  & & & &  \\
\hline%\hline
\multicolumn{5}{@{}l}{\scriptsize{\textit{Contextual Embeddings}}}\\
Flair-official & & & &  \\
Flair-BMC  & & & &  \\
%\hline
%\multicolumn{5}{@{}l}{\scriptsize{\textit{BERT Embeddings}}}\\
mBERT-official  & & & &  \\
BERT-BMC  & & & &  \\
\hline 
\multicolumn{5}{@{}l}{\scriptsize{\textit{Previous state-of-the-art}}} \\
\hline
\end{tabular}
\caption{Summary table across all tasks. Micro F1 scores are reported}\label{sec:results-discussion:table}
\end{table*}

\todo[inline]{discussion over summary table}
% - discussion

\section{Conclusions and future work}
\label{sec:concl-future-work}


\bibliographystyle{lrec}
\bibliography{main}

\end{document}
