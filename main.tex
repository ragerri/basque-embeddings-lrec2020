\documentclass[10pt, a4paper]{article}
\usepackage{lrec}
%\usepackage{multibib}
%\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}
\usepackage{todonotes}
% for eps graphics

\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{xstring}

\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}

% google doc: https://docs.google.com/document/d/1czT8UndVGbKUfSRZtaHVyWbpFcGSV33USwytefEv4jM/edit?usp=sharing


\title{Build-your-own is better than public text representation models: evidence for the Basque Language}

\name{Author1, Author2, Author3}

\address{Affiliation1, Affiliation2, Affiliation3 \\
         Address1, Address2, Address3 \\
         author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\
         \{author1, author5, author9\}@abc.org\\}


\abstract{
% Ondorio nagusiak:
% Euskararako bert propioa > mbert publikoa eta euskararako fastext propioa > fastext publikoa eta flair propioa > flair publikoa
% Atazak: topic class / polarity / ner / pos
% Artearen egoera ataza guztietan (konprobatu bakoitzak berea)
% Euskaraz: bert > flair > fastext (ñabardurekin idatzi)
% Embedding-ak eta ereduak banatuko ditugu
% Honek ñabardura gehitzen dio alemanari (japonesa?) buruzko lanari (jon ander) \\ \newline \Keywords{keyword1, keyword2, keyword3}
}

\begin{document}

\maketitleabstract

\section{Introduction}\label{sec:introduction}

\todo[inline]{abstract/intro enekok}


\section{Related work}\label{sec:related-work}

% embedding / bert / mbert
\todo[inline]{related work aitor hasi. camembert aipatu?}


\section{Building Basque models}\label{sec:build-basq-models}

% - Collecting corpora: iñaki
\subsection{Collecting Corpora}\label{sec:build-basq-models:corpora}
 
 We collected a corpus comprising the Basque Wikipedia\footnote{The dump from 2019/10/01 was used.}, and news articles crawled from Basque media. In total it contains 226 million tokens, with 35 millions coming from Wikipedia. We will refer to this corpus as Basque Media Corpus (BMC).
 
 \todo[inline]{Justificatu zergatik ez common crawl. Aipatu iturriak. justifikatu zergatik ezin dugun publiko jarri}


 
% - fastext iñaki/xabier
\subsection{Static Embeddings: FastText}\label{sec:build-basq-models:static}

To the best of our knowledge, the only publicly available static word embeddings for basque are those distributed by Facebook computed from two sources by using The FastText algorithm \cite{}:
\begin{itemize}
    \item Wikipedia (wiki-ft) based embeddings \cite{fasttext1_bojanowski2017enriching} (skipgram,d300,mf5,char3\_6,w5,e5,negs5)
    \item Common Crawl and Wikipedia based embeddings (cc-ft) \cite{fasttext2_grave2018learning}
\end{itemize} and cc-wiki (cbow,char\_5-5,w5,e5,negs10)
Ours: default original paper (ref. needed) -  (cbow,d300,mf5,char3\_6,w5,e5,negs5)
Ablation tests?

Mention to word2vec?


% - Flair rodri
\subsection{Character-based Contextual Embeddings}\label{sec:build-basq-models:flair}

Static word embeddings such as FastText \cite{fasttext1_bojanowski2017enriching} provide a unique vector-based representation for a given word independently of the context in which the word occurs. Thus, if we consider the Basque word \emph{banku}\footnote{English: bank.}, static word embedding approaches will calculate one vector irrespective of the fact that the same word \emph{banku} may convey in some contexts different senses, namely, ``financial institution'',``bench'', ``supply or stock'', among others. This problem has been recently addressed by the so-called contextual word embeddings with the objective of being able to generate different word representations according to the context in which the word appears. Example of such contextual representations are ELMO \cite{Peters:2018} and Flair \cite{akbik2018coling}.

Flair refers to both a deep learning toolkit and to a specific type of character-based contextual word embeddings. Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks \cite{akbik2018coling}, outperforming other well-known approaches such as BERT and ELMO \cite{devlin2018bert,Peters:2018}. In any case, Flair is of interest to us because they distribute their own Basque pre-trained embedding models obtained from a corpus of 249M words.

In the Flair approach, words are modeled as sequences of characters that are passed to a character-based language model in order to generate word representations. More specifically, they use two LSTM models (backwa)


For best results, the Flair authors recommend to combine their own Flair embeddings with additional static embeddings such as FastText. Thus, we follow their advice and use their system leveraging the pre-trained word embedding and meta-embedding models combined with the Flair character-based contextual embeddings for English and Spanish.


% - Bert ander/jon ander
\subsection{BERT language models}\label{sec:build-basq-models:bert}



Sarreratxoa ataleko kontu nagusiak azalduz. 

\paragraph{Model Architecture}

In the same way as the original BERT architecture proposed by \newcite{devlin2018bert} our model is composed by stacked layers of Transformer encoders \cite{vaswani2017attention}. Our approach follows the BERT\textsubscript{BASE} configuration containing 12 Transformer encoder layers, a hidden size of 768 and 12 self-attention heads for a total of 110M parameters. 

Gainetik - BERT base erabiltzen dugula esan - transformer encoderra ezabili.

\paragraph{Pretraining objective}

MaskedLM and NSP azaldu//Whole work masking - Arazoa NSP ez da erabiltzen azken modeloetan RoBERTA, XLM...


\paragraph{Vocabulary}

SentencePiece azaldu,  gure vocab size 50K

\paragraph{Pretraining procedure}

Optimization aipatu, Cloud v2 TPUs for 128 length, v3 for 512 length - 90pstep 128length 10pstep 512 length - total 100000 step 3 egun


\section{Experimental settings}\label{sec:exper-sett}

% - Datasets iñaki/xabier/rodri
We conduct an extensive evaluation on four well known NLP tasks: Topic Classification, Sentiment Classification, POS tagging, and Named Entity Recognition (NER). POS tagging and NER are evaluated on two establish benchmarks, i.e., Universal Dependencies and EIEC corpus. 

For polarity and topic classification we introduce two new datasets for Basque. For sentiment classification a corpus of tweets containing messages related to the cultural domain was used \cite{san2019multilingual}\footnote{The dataset is publicly available at https://hizkuntzateknologiak.elhuyar.eus/assets/files/behaguneadss2016-dataset.tgz}. The corpus contains three class annotations (positive, negative and neutral). For the task of topic classification a dataset containing 12k news short texts was compiled. Each text is tagged as belonging to a single topic in a 12 class scheme


% - Text classification models iñaki/xabier
% - Sequence labeling models iñaki/rodri/jon ander

\section{Results and discussion}\label{sec:results-discussion}

nomenclatura: \begin{itemize}
    \item fastext-official-wikipedia 
    \item fastext-official-common-crawl 
    \item fastext-BMC
    \item flair-official
    \item flair-BMC
    \item mBERT-official
    \item BERT-BMC
\end{itemize}

\todo[inline]{ataza bakoitzeko taula bat eta atal bat, emaitza horiek komentatzen, artearen egoera barne.}


% - Topic classification xabier
% - Polarity iñaki
% - PoS tagging rodri/iñaki
% - NERC rodri

\subsection{POS Tagging}\label{sec:pos-tagging}

In order to facilitate comparison with previous state-of-the-art methods, we experiment with the Universal Dependencies 1.2 data, which provides train, development and test partitions. We train the Flair off-the-shelf using the parameters used in Akbik et al. \cite{akbik2018coling}, tuning the system on the development data and using the test only for the final evaluation. Table shows first the results obtained by training the system with the Flair character-based contextual embeddings only. After that we combine the Flair embeddings with FastTex embeddings (both official and BMC).

Furthermore, our best results establish new state-of-the-art performance for POS tagging using UD 1.2 for Basque.


\subsection{Named Entity Recognition}\label{sec:named-entity-recogn}



\begin{table*}[!t]\scriptsize
\centering
\begin{tabular}{@{\hspace{0.3cm}}lcccc} \hline
\textbf{} & \multicolumn{4}{c}{\textbf{Task}} \\ %\hline 
 & {\textbf{Topic Classification}} & {\textbf{Polarity}} &  {\textbf{POS}} & {\textbf{NER}}\\ \hline
\multicolumn{5}{@{}l}{\textit{\scriptsize{Static Embeddings}}} \\
fastext-wikipedia & & & & \\
fastext-common-crawl & & & &  \\
fastext-BMC  & & & &  \\
\hline%\hline
\multicolumn{5}{@{}l}{\scriptsize{\textit{Contextual Embeddings}}}\\
Flair-official & & & &  \\
Flair-BMC  & & & &  \\
%\hline
%\multicolumn{5}{@{}l}{\scriptsize{\textit{BERT Embeddings}}}\\
mBERT-official  & & & &  \\
BERT-BMC  & & & &  \\
\hline 
\multicolumn{5}{@{}l}{\scriptsize{\textit{Previous state-of-the-art}}} \\
\hline
\end{tabular}
\caption{Summary table across all tasks. Micro F1 scores are reported}\label{sec:results-discussion:table}
\end{table*}

\todo[inline]{discussion over summary table}
% - discussion

\section{Conclusions and future work}\label{sec:concl-future-work}


\bibliographystyle{lrec}
\bibliography{main}

\end{document}
